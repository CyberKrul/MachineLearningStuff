{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33434e12-1fe6-4990-b570-784c0dedccbb",
   "metadata": {},
   "source": [
    "# Time Series Theory 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10cd10-3fa7-4d9e-9f3b-4dc96f076e9d",
   "metadata": {},
   "source": [
    "## Moving Average (MA) Processes\n",
    "* $Y_t = \\Sigma_{k=1}^P\\theta_ke_{t-k}$, with $\\theta_0=1$, and $P$ is the **Model Order**, which tells us how many coefficients are in the model\n",
    "* What are the unknowns in the MA model? They are $P$ and $\\bar{\\theta}$, the coefficient (here represented with a vector)\n",
    "* For a given $P$, the variance $\\text{Var}(Y_t)=P\\sigma_e^2$- if $P$ is fixed, then variance is fixed. Therefore this is a stationary process for fixed $P$\n",
    "* Say we have $P=2$, then,\n",
    "  - $Y_t=\\theta_0e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2}=e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2}$\n",
    "  - where $e_t\\sim\\mathcal{N}(0,\\sigma^2)$ and all $e$ are iid. $\\theta_0=1$\n",
    "* Say the mean of the process will still be $\\mu_{Y_t}=0$, and the variance can be calculated as: $\\text{Var}(Y_t) = \\text{Var}(e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2})=\\text{Var}(e_t)+\\theta_1^2\\text{Var}(e_{t-1})+\\theta_2^2\\text{Var}(e_{t-2})$\n",
    "  - Therefore, we get the variance as: $\\sigma_e^2+\\theta_1^2\\sigma_e^2+\\theta_2^2\\sigma_e^2$\n",
    "\n",
    "- Therefore the mean and variance are given by:\n",
    "  * $E[Y_t]=0$\n",
    "  * $\\text{Var}(Y_t) =\\sigma_e^2\\Sigma_{k=0}^q\\theta_k^2$\n",
    "---\n",
    "Therefore, this is a stationary process, as the mean and variance are constant\n",
    "\n",
    "---\n",
    "**EXAMPLE**\n",
    "* Suppose you win 1 Dollar if a fair coin shows a head and lose 1 dollar if it shows a tail. Denote the outcome on toss $t$ by $a_t$\n",
    "* $e_t=\\left\\{\\begin{array}{ll} 1 & \\text{if head}\\\\ -1 & \\text{if tail}\\end{array}\\right.$\n",
    "* The average $Y_t$ winning from 4 tosses: $Y_t = \\frac{1}{2}e_t+\\frac{1}{2}e_{t-1}+\\frac{1}{2}e_{t-2}+\\frac{1}{2}e_{t-3}$\n",
    "* This is a moving average process\n",
    "\n",
    "- Notice that the observed series $Y_t$ is autocorrelated even though the generating series $e_t$ is uncorrelated\n",
    "- The series $Y_t$ is the weighted aggregation of some uncorrelated random variables\n",
    "- In Economics, the generating series, $e_t$ is called the random shock\n",
    "\n",
    "---\n",
    "## MA[2] process\n",
    "##### Lag of 0\n",
    "* An MA[2] process is $Y_t=e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2}$\n",
    "* The ACF of this process, $\\gamma_{Y_t,Y_{t+0}}=\\gamma_0=E[(Y_t-\\mu_y)(Y_{t-0}-\\mu_y)]$\n",
    "* $\\gamma_0 = E[(e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2})(e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2})]$\n",
    "* $=E[e_t^2+\\theta_1^2e_{t-1}^2+\\theta_2^2e_{t-2}^2+2(\\theta_1e_te_{t-1}+\\theta_2e_te_{t-2}+\\theta_1\\theta_2e_{t-1}e_{t-2})]$\n",
    "* $=E[e_t^2+\\theta_1^2e_{t-1}^2+\\theta_2^2e_{t-2}^2]$, as $e_t$ is iid\n",
    "* $=\\sigma_e^2(1+\\theta_1^2+\\theta_2^2)$\n",
    "\n",
    "\n",
    "##### Lag of 1\n",
    "- The ACF of this process, $\\gamma_{Y_t,Y_{t+1}}=\\gamma_1=E[(Y_t-\\mu_y)(Y_{t-1}-\\mu_y)]$\n",
    "- $\\gamma_1 = E[(e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2})(e_{t-1}+\\theta_1e_{t-2}+\\theta_2e_{t-3})]$\n",
    "- $=E[e_te_{t-1}+\\theta_1e_te_{t-2}+\\theta_2e_te_{t-3}+\\theta_1e_{t-1}^2+\\theta_1^2e_{t-1}e_{t-2}+\\theta_1\\theta_2e_{t-1}e_{t-3}+\\theta_2e_{t-2}e_{t-1}+\\theta_2\\theta_1e_{t-2}^2+\\theta_2^2e_{t-2}e_{t-3}]$\n",
    "- $=E[\\theta_1e_{t-1}^2+\\theta_1\\theta_2e_{t-2}^2]$, as $e_t$ is iid\n",
    "- $=\\sigma_e^2(\\theta_1+\\theta_1\\theta_2)$\n",
    "\n",
    "##### Lag of 2\n",
    "*  The ACF of this process, $\\gamma_{Y_t,Y_{t+2}}=\\gamma_2=E[(Y_t-\\mu_y)(Y_{t-2}-\\mu_y)]$\n",
    "* $\\gamma_2 = E[(e_t+\\theta_1e_{t-1}+\\theta_2e_{t-2})(e_{t-2}+\\theta_1e_{t-3}+\\theta_2e_{t-4})]$\n",
    "* $=E[e_te_{t-2}+\\theta_1e_te_{t-3}+\\theta_2e_te_{t-4}+\\theta_1e_{t-1}e_{t-2}+\\theta_1^2e_{t-1}e_{t-3}+\\theta_1\\theta_2e_{t-1}e_{t-4}+\\theta_2e_{t-2}^2+\\theta_2\\theta_1e_{t-2}e_{t-3}+\\theta_2^2e_{t-2}e_{t-4}]$\n",
    "* $=E[\\theta_2e_{t-2}^2]$, as $e_t$ is iid\n",
    "* $=\\sigma_e^2\\theta_2$\n",
    "\n",
    "##### Lag of 3 and onwards\n",
    "$\\gamma_k=0,\\:k\\in[3,\\infty)$ in a MA[2] process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdea6fb-af22-4065-b72e-7a1ef26cc09f",
   "metadata": {},
   "source": [
    "## Finding the values of $\\bar{\\theta}$\n",
    "* $Y_t = e_t+\\theta_1y_{t-1}$, an MA[1] process\n",
    "* $Y_t$ is known, $e_t$ is unknown, but $e_t\\sim\\mathcal{N}(0,\\sigma^2)$, and is iid\n",
    "* From here we have two paths forward:\n",
    "  - Iterative Least Squares\n",
    "    1. Assume $e_t$ ($e_{t-1}$ values are assumed)\n",
    "    2. Fit a least squares model between $e_t$ and $Y_t$\n",
    "    3. Compile residuals: $\\xi_t=Y_t-\\hat{Y}_t$, $\\hat{Y}_t$ is from the Least Squares Model\n",
    "    4. Let $e_t = \\xi_t$. Iterate steps 2 and 3\n",
    "    5. Stop when ACF of $\\xi_t$ is like white noise OR $n$ iterations\n",
    "\n",
    "---\n",
    "\n",
    "* If the $e_t$s are normal, then so is the process, which is strictly stationary.\n",
    "* The autocorrelation is:\n",
    "\n",
    "$\\rho_k = \\left\\{\\begin{array}{ll}1 & \\text{if}\\: k= 0\\\\\\Sigma_{i=1}^{q-k}\\theta_i\\theta_{i+k}/\\Sigma_{i=1}^q\\theta_i^2 & \\text{if}\\:k=1,2,\\dots,q \\\\ 0 & \\text{if}\\: k\\gt q \\\\ \\rho_{-k} & \\text{if}\\:k\\lt 0\\end{array}\\right.$\n",
    "\n",
    "* The process is weakly stationary because the mean is constant and the covariance does not depend on $t$. Note that it cuts off at lag $q$\n",
    "\n",
    "---\n",
    "### Moving Average Processes\n",
    "* For general processes introduce the backward shift operator $B$, $B^jy_t=y_{t-j}$\n",
    "* Then the MA($q$) process is given by\n",
    "  - $Y_t = (\\theta_0+\\theta_1B+\\theta_2B^2+\\dots+\\theta_qB^{2q}=\\theta(B)e_t$\n",
    "\n",
    "---\n",
    "## MA: Stationarity\n",
    "* In general, MA processes are stationary regardless of the values of the parameters, but not necessarily 'invertible'\n",
    "* An MA process is said to be invertible if it can be converted into a stationary AR process of infinite order\n",
    "* In order to ensure there is a unique MA process for a given ACF, we impose the condition of invertibility\n",
    "* Therefore, invertibility condition for MA process servers two purposes:\n",
    "  1. It is useful to represent an MA process as an infinite order AR process;\n",
    "  2. It ensures that for a given ACF, there is a unique MA process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ae923-2d74-4068-82f3-5ed617dbe205",
   "metadata": {},
   "source": [
    "## Auto Regressive Processes\n",
    "* Assume $\\{e_t\\}$ is purely random with mean zero and std $\\sigma_e$\n",
    "* Then the AR process of order $p$ or AR($p$) is:\n",
    "\n",
    "$Y_t = \\phi_1y_{t-1}+\\phi_2t_{t-2}+\\dots+\\phi_py_{t-p}+e_t$\n",
    "\n",
    "$Y_t = \\Sigma^p\\phi_ky_{t-k} + e_t$\n",
    "\n",
    "- What is the relation between AR and MA?\n",
    "  * Let's define $B$ as the backshift operator, $B^jy_t = y_{t-j}$\n",
    "  * So we can rewrite the AR equation as\n",
    "    - $Y_t = \\Sigma^p\\phi_kB^ky_t +e_t$\n",
    "  * Using algebra to shift the $y_t$ terms on one side, we get\n",
    "    - $y_t[\\Sigma^p\\phi_kB^k] = e_t$\n",
    "  * $\\frac{y_t}{e_t}=1/[1-\\Sigma^p\\phi_kB^k]$\n",
    "  * For an MA process, $y_t = e_t+\\Sigma^q e_{t-k}\\theta_k$\n",
    "     - $\\frac{y_t}{e_t}=1+\\Sigma^q\\theta_kB^k$\n",
    " * Their relationship in the extremes, and they complement each other\n",
    "\n",
    "---\n",
    "#### Finding first two moments of the AR[1] process\n",
    "$y_t =\\Sigma_{k=1}^p y_{t-k}\\phi_k + e_t$, the AR process\n",
    "\n",
    "$\\mu_y =0$, $\\gamma_{Y_t,Y_{t+0}}=\\gamma_0=\\sigma_y^2$ and $e_t\\sim\\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "Let's take an AR[1] process: $Y_t = \\phi_1y_{t-1} +e_t$\n",
    "\n",
    "$\\sigma_y^2 = E[(\\phi_1y_{t-1}+e_t)(\\phi_1y_{t-1}+e_t)]= E[\\phi_1^2y_{t-1}^2+e_t^2+2\\phi_1y_{t-1}e_t]$\n",
    "\n",
    "Given $y_{t-1}=y_{t-2}\\phi_1+e_{t-1}$, we can say theres no linear relation between $y_{t-1}$ and $e_t$ as $e_t$ and $e_{t-1}$ are iid\n",
    "\n",
    "$\\sigma_y^2= \\phi_1^2\\sigma_y^2 + \\sigma_e^2 + 0\\rightarrow \\sigma_y^2 = \\sigma_e^2/(1-\\phi_1^2)$\n",
    "\n",
    "#### Autocovariance of the AR[1] process (lag1)\n",
    "$\\gamma_1 = E[(Y_t-\\mu_y)(Y_{t-1}-\\mu_y)]=E[(\\phi_1y_{t-1}+e_t-\\mu_y)(\\phi_1y_{t-2}+e_{t-1}-\\mu_y)]$\n",
    "\n",
    "$\\gamma_1 = E[\\phi_1^2y_{t-1}y_{t-2}+e_te_{t-1}+\\phi_1y_{t-1}e_{t-1}+\\phi_1y_{t-2}e_t]$\n",
    "\n",
    "$\\gamma_1 = \\phi_1^2\\gamma_1+0 + \\phi_1\\sigma_e^2+0\\rightarrow \\gamma_1 = \\phi_1\\sigma_e^2/(1-\\phi_1^2)$\n",
    "\n",
    "As $\\phi_1$ is typically less than one, the autocovariance is reduced by a factor of $\\phi_1$ in an AR[1] process\n",
    "$\\gamma_1 = \\phi_1\\gamma_0=\\phi_1\\sigma_y$\n",
    "\n",
    "#### Autocovariance of the AR[1] process (lag2)\n",
    "$\\gamma_2 = E[(Y_t-\\mu_y)(Y_{t-2}-\\mu_y)]=E[(\\phi_1y_{t-1}+e_t-\\mu_y)(\\phi_1y_{t-3}+e_{t-2}-\\mu_y)]$\n",
    "\n",
    "$\\gamma_2 = E[\\phi_1^2y_{t-1}y_{t-3}+e_te_{t-2}+\\phi_1y_{t-1}e_{t-2}+\\phi_1y_{t-3}e_t]$\n",
    "\n",
    "if we expand the equation for $y_{t-1}$, we get $y_{t-1} =\\phi_1(\\phi_1y_{t-3}+e_{t-2})+e_{t-1}$\n",
    "\n",
    "$\\gamma_2 = \\phi_1^2\\gamma_2+0 +\\phi_1^2\\sigma_e^2+0\\rightarrow \\gamma_2 = \\phi_1^2\\sigma_e^2/(1-\\phi_1^2)$\n",
    "\n",
    "As $\\phi_1$ is typically less than one, the autocovariance is reduced by a factor of $\\phi_1^2$ in an AR[1] process\n",
    "$\\gamma_2 = \\phi_1^2\\gamma_0=\\phi_1\\sigma_y$\n",
    "\n",
    "\n",
    "Similarly, $\\gamma_3 = \\phi_1^3\\sigma_e^2/(1-\\phi_1^2)$, and so on\n",
    "\n",
    "---\n",
    "* Since the ACF will never go to zero, we cannot use ACF to determine the model order of an AR process\n",
    "* We see there's an indirect relationship between $y_t$ and $y_{t-k}$, in the AR model\n",
    "* ACF cannot distinguish between the indirect and direct  pathways\n",
    "* For that, we use PACF, so we only get the direct effect, by removing any indirect relationships, then applying ACF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a765efa-1a1a-4804-be8b-e0a64ec740d8",
   "metadata": {},
   "source": [
    "### Steps to calculate PACF\n",
    "NOTE: $\\hat{\\alpha},\\hat{\\beta} = \\rho_1$\n",
    "1. Use the Least Squares method to model $\\hat{y}_t = \\hat{\\alpha}y_{t-1}$\n",
    "2. Use the Least Squares method to model $\\hat{y}_{t-2} = \\hat{\\beta}y_{t-1}$\n",
    "3. Compute residuals:\n",
    "   - $\\xi_t = y_t-\\hat{y}_t$, $\\:\\xi_t$ will not have any effect on $y_{t-1}$\n",
    "   - $\\xi_{t-2} = y_{t-2}-\\hat{y}_{t-2}$, $\\:\\xi_{t-2}$ will not have any effect on $y_{t-1}$\n",
    "4. PACF[2]$ =\\Phi_2 = \\text{Corr}(\\xi_t,\\xi_{t-2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2311f9-820d-41b6-857d-15ceda39794e",
   "metadata": {},
   "source": [
    "## ARMA process\n",
    "* Combine AR and MA processes\n",
    "* *A random process that contains both auto-regressive and moving average parts is said to be an ARMA($L_{AR},L_{MA}$) process*\n",
    "* An ARMA process of order $(p,q)$ is given by:\n",
    "\n",
    "$Y_t = \\alpha_1y_{t-1}+\\dots+\\alpha_py_{t-p}+e_t+\\beta_1e_{t-1}+\\dots+\\beta_qe_{t-q}$\n",
    "\n",
    "Alternate expressions are possible using the backshift operator\n",
    "\n",
    "$\\Phi(B)y_t = \\Theta(B)e_t$\n",
    "\n",
    "* where\n",
    "  - $\\Phi(B)=1+\\alpha_1B+\\dots+\\alpha_pB^p$\n",
    "  - $\\Theta(B) = 1+\\beta_1B+\\dots+\\beta_qB^q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20fdb7-9986-43b2-86d7-12c60068c66e",
   "metadata": {},
   "source": [
    "<img src='acpacf.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ad227-796e-4224-871a-4b511a509666",
   "metadata": {},
   "source": [
    "## ARIMA processes\n",
    "* General autoregressive integrated moving average processes are called ARIMA processes\n",
    "* Typically used on non-stationary time series\n",
    "* When differenced, say $d$ times, the process is an ARMA process\n",
    "* Call the differentiated process $W_t$. then $W_t$ is an ARMA process and\n",
    "  - $W_t(\\text{ARMA})=\\Delta^dy_t(\\text{ARIMA})=(1-B)^dy_t$\n",
    " \n",
    "* It has 3 terms: $p,d,q$\n",
    "  - $p$ is the AR term\n",
    "  - $d$ is the difference order\n",
    "  - $q$ is the MA term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97c6e2-0004-4525-a4fc-63444fe347cb",
   "metadata": {},
   "source": [
    "## Box-Jenkins Methods (ARIMA models)\n",
    "* The Box-Jenkins methodology refers to a set of procedures for identifying and estimating time series models within the class of autoregressive integrated moving average (ARIMA) models\n",
    "* ARIMA models are regression models that use lagged values of the dependent variable and/or random disturbance terms as explanatory variables\n",
    "* ARIMA models rely heavily on the autocorrelation pattern in the data\n",
    "* This method applies to both seasonal and non-seasonal data\n",
    "\n",
    "---\n",
    "### Box-Jenkins methods- 5 Steps\n",
    "1. Stationarity checking and Differencing- extracting the non-stationarity and removing trends\n",
    "2. Model Identification - is it an AR, MA or ARMA (also identify order)\n",
    "3. Parameter Estimation - find the $\\phi$s or $d$s\n",
    "4. Diagnostic Checking - validate the model\n",
    "5. Forecasting - predict the required data\n",
    "\n",
    "---\n",
    "#### Differencing\n",
    "1. If the process is non-stationary, then first differences of the series are computed to determine if that operation results in a stationary series\n",
    "2. The process is continued until a stationary time series is found\n",
    "3. This then determines the value of $d$\n",
    "4. Sometimes, transformations, like log or some variance stabilizing transformations are made before differencing\n",
    "\n",
    "---\n",
    "#### Diagnostic Checking\n",
    "* Often it is not straightforward to determine a single model that most adequately represents the data generating process, and it is not uncommon to estimate several models at the inital stage. The model that is finally chosen is the one considered best based on a set of diagnostic checking criteria. These criteria include:\n",
    "  - t-tests for coefficient significance\n",
    "  - residual analysis\n",
    "  - model selection criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abd9be-7408-4bdd-892f-bb3d1264cf98",
   "metadata": {},
   "source": [
    "### Model Selection Criterion\n",
    "* Akaike Information Criterion (AIC)\n",
    "  - AIC $=-2\\ln(L)+2k$\n",
    "* Schwartz Bayesian Criterion (SBC)\n",
    "  - SBC $-2\\ln(L)+k\\ln(n)$\n",
    "* Here,\n",
    "  - $L$ is the Likelihood function\n",
    "  - $k$ is the number of parameters to be estimated\n",
    "  - $n$ is the number of observations\n",
    "*  Ideally, the AIC and SBC should be as small as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
