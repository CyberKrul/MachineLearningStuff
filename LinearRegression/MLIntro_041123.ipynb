{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d03db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e0f14",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "## What is Machine Learning?\n",
    "* It is a big data phenomenon due to collection and storage of data at an unprecedented rate\n",
    "* ML uses computers to automatically detect patterns in data and make predictions or decisions\n",
    "* It is a subset of Artificial Intelligence (AI)\n",
    "\n",
    "* Machine Learning Algorithms are able to learn information directly from data without relying on a predetermined mathematical model\n",
    "![](MLvenn.jpeg)\n",
    "\n",
    "Another image incorporating Data Science/Big data\n",
    "![](AIvenn.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287aa3f",
   "metadata": {},
   "source": [
    "#### There are 3 main Learning Paradigms:\n",
    "1. **Supervised Learning**: Given a set of input patterns $X$ and a corresponding set of label $Y$, the underlying mapping function $f:X\\rightarrow Y$ is discovered. The classifier operates in two distinct phases, 1-> a training phase (model tuning), and an operating phase, in which the model is kept fixed and tested with different and new data.<br>\n",
    "2. **Unsupervised Learning**: Given a set of data $X$ with no labels, the underlined structure in the data is discovered to provide an internal representation.<br>\n",
    "3. **Reinforcement learning**: The network is used to control a system (actor) by generating actions according to the current system state. The actions chosen by the actor are evaluated by another network (critic) which generates a reinforcement signal (a reward or punishment) for the actor, in order to improve its performace.\n",
    "\n",
    "![](supunsup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051ea5d",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "* The number of columns, also called the *features*, in the table below is $d=3$\n",
    "* The number of datapoints, also called *training examples* is $n=20$\n",
    "\n",
    "- Features:\n",
    "    * Inhabitants: Number of inhabitants\n",
    "    * BelowIncome: the percentage of families with incomes below 5000 dollars\n",
    "    * PercentageUnemployed: the percentage unemployed\n",
    "- Target:\n",
    "    * Murders: the number of murders per 1 million inhabitants per annum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f692d663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inhabitants</th>\n",
       "      <th>BelowIncome</th>\n",
       "      <th>PercentageUnemployed</th>\n",
       "      <th>Murders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>687000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123445</td>\n",
       "      <td>20.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127345</td>\n",
       "      <td>26.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>40.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>867345</td>\n",
       "      <td>16.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283457</td>\n",
       "      <td>16.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>123456</td>\n",
       "      <td>18.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>235869</td>\n",
       "      <td>20.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>783753</td>\n",
       "      <td>21.2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>35.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>875934</td>\n",
       "      <td>17.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>456789</td>\n",
       "      <td>15.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>687000</td>\n",
       "      <td>18.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>123445</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>127345</td>\n",
       "      <td>19.1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>867345</td>\n",
       "      <td>25.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>283457</td>\n",
       "      <td>18.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    inhabitants  BelowIncome  PercentageUnemployed  Murders\n",
       "0        687000         16.5                   6.2     11.2\n",
       "1        123445         20.5                   6.4     13.4\n",
       "2        127345         26.3                   9.3     40.7\n",
       "3        867345         16.5                   5.3      5.3\n",
       "4        283457         16.2                   7.3     14.8\n",
       "5        123456         18.5                   5.8     12.7\n",
       "6        235869         20.2                   6.4     20.8\n",
       "7        783753         21.2                   7.4     35.7\n",
       "8        875934         17.2                   4.9      8.7\n",
       "9        456789         15.2                   6.4      9.5\n",
       "10       687000         18.1                   6.0     15.5\n",
       "11       123445         23.1                   7.4     26.9\n",
       "12       127345         19.1                   6.8     15.2\n",
       "13       867345         25.7                   8.8     25.2\n",
       "14       283457         18.6                   6.5     18.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(index = np.arange(0,20),\n",
    "                 data = {\n",
    "                     'inhabitants':[687000,123445,127345,867345,283457,123456,235869,783753,875934,456789,687000,123445,127345,867345,283457,123456,235869,783753,875934,456789],\n",
    "                     'BelowIncome':[16.5,20.5,26.3,16.5,16.2,18.5,20.2,21.2,17.2,15.2,18.1,23.1,19.1,25.7,18.6,24.7,17.0,22.5,20.2,16.9],\n",
    "                     'PercentageUnemployed':[6.2,6.4,9.3,5.3,7.3,5.8,6.4,7.4,4.9,6.4,6.0,7.4,6.8,8.8,6.5,8.3,8.7,8.0,8.4,6.7],\n",
    "                     'Murders':[11.2,13.4,40.7,5.3,14.8,12.7,20.8,35.7,8.7,9.5,15.5,26.9,15.2,25.2,18.1,26.8,15.3,25.8,21.7,25.7]\n",
    "                 })\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf32c8",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "Let's consider 3 training examples\n",
    "\n",
    "|Index| Inhabitants|BelowIncome|PercentageUnemployed|Murders|\n",
    "|-----|------------|-----------|--------------------|-------|\n",
    "|0|587000|16.5|6.2|11.2|\n",
    "|1|643000|20.5|6.4|13.4|\n",
    "|2|635000|26.3|9.3|40.7|\n",
    "\n",
    "We can then define the *feature vectors* and the *targets* as:<br>\n",
    "$x_1=[587000,16.5,6.2];\\hspace{10pt}y_1=11.2$<br>\n",
    "$x_2=[643000,20.5,6.4];\\hspace{10pt}y_2=13.4$<br>\n",
    "$x_3=[635000,26.3,9.3];\\hspace{10pt}y_3=40.7$\n",
    "* $x_i$ is the *feature vector* for example $i$\n",
    "* $y_i$ is the *target* for example $i$\n",
    "\n",
    "Our Goal is to find the mapping $\\mathbf{\\mathcal{f}}$ such that:<br>\\\n",
    "$\\hspace{20pt} y_i=\\mathbf{\\mathcal{f}}(x_i)\\hspace{10pt}$for $i\\in[0,19]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92264a3d",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "When $y_i$ is continuous, this is a regression problem\n",
    "\n",
    "### Supervised Learning: Regression\n",
    "Let's consider the **linear regression** problem.\n",
    "___\n",
    "Feature vector: $\\hspace{10pt}x_i=[x_{i1},x_{i2},\\dots,x_{id}]$\n",
    "\n",
    "Target: $\\hspace{10pt}y_i$\n",
    "\n",
    "Prediction: $\\hspace{10pt}\\hat{y}_i = w_1x_{i1}+w_2x_{i2}+\\dots+w_dx_{id}\\hspace{10pt}$where,<br>\n",
    "\n",
    "* $w_i$ is the weight of feature $i$\n",
    "* $x_{ij}$ is the $j$-th feature of the $i$-th datapoint\n",
    "* $\\hat{y}_i$ which is the **weighted sum of features** of the $i$-th datapoint\n",
    "___\n",
    "To make the prediction be close to the target value, we try to minimize $y_i-\\hat{y}_i$\n",
    "___\n",
    "We can expand the *residual* $y_i-\\hat{y}_i$ as:\n",
    "\n",
    "$\\hspace{20pt}y_i-\\hat{y}_i = y_i-\\Sigma_{j=1}^d w_jx_{ij}$\n",
    "\n",
    "We need to tune the weights such that the prediction error or residual is minimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ea598",
   "metadata": {},
   "source": [
    "### Minimizing the Least Square Error\n",
    "What is the classic way to minimize this error considering we have $n$ training examples?\n",
    "___\n",
    "We can minimize the **Least Square Error**\n",
    "___\n",
    "We find $\\bar{w}$ such that it minimizes $\\Sigma_{i=1}^n(y_i-\\hat{y}_i)^2$<br>where, $\\bar{w}=[w_1,w_2,\\dots,w_d]$\n",
    "\n",
    "Expanding the summation, we get:\n",
    "\n",
    "$\\Sigma_{i=1}^n(y_i-\\hat{y}_i)^2 = \\Sigma_{i=1}^n(y_i-\\bar{w}^Tx_i)^2=\\lVert\\mathbf{X}\\bar{w}-\\bar{y}\\rVert^2$\n",
    "* $\\bar{w}$ is an unknown $d\\times 1$ column vector, called the *weight vector*\n",
    "* $\\bar{y}$ is a known $n\\times 1$ column vector, called the *target vector*\n",
    "* $\\mathbf{X}$ is a known $n\\times d$ matrix, called the *feature matrix*\n",
    "---\n",
    "\n",
    "We find a solution by optimizing as:<br> \n",
    "<p style=\"text-align: center;\"> \n",
    "$minimize\\hspace{2pt} \\lVert \\mathbf{X}\\bar{w}-\\bar{y}\\rVert^2\\hspace{2pt}$ w.r.t $\\hspace{2pt}\\bar{w}\\in\\mathbb{R}^d$\n",
    "</p>\n",
    "\n",
    "* We must find the gradient, set it to zero, and solve for $\\bar{w}$\n",
    "\n",
    "* The gradient, which is just a vector of partial derivatives, is given by- $\\nabla f(\\bar{w}) = [\\frac{\\partial f}{\\partial w_1},\\frac{\\partial f}{\\partial w_2},\\dots,\\frac{\\partial f}{\\partial w_d}]^T$\n",
    "\n",
    "* We have framed the *Least Squares problem* as: $minimize\\hspace{2pt}\\lVert \\mathbf{X}\\bar{w}-\\bar{y}\\rVert^2\\hspace{2pt}$ w.r.t $\\hspace{2pt}\\bar{w}\\in\\mathbb{R}^d$\n",
    "\n",
    "* An equivalent problem, which will reduce some calculations for us, is: $minimize\\hspace{2pt}\\frac{1}{2}\\lVert \\mathbf{X}\\bar{w}-\\bar{y}\\rVert^2\\hspace{2pt}$ w.r.t $\\hspace{2pt}\\bar{w}\\in\\mathbb{R}^d$\n",
    "\n",
    "* Therefore, we get (**NOTE**, the bars for the vectors are ignored for clarity)\n",
    "\n",
    "$\\mathcal{f}(\\bar{w}) =\\frac{1}{2}\\lVert \\mathbf{X}\\bar{w}-\\bar{y}\\rVert^2 = \\frac{1}{2}(\\mathbf{X}w-y)^T(\\mathbf{X}w-y)$\n",
    "\n",
    "$\\hspace{79pt}=\\frac{1}{2}(w^T\\mathbf{X}^T-y^T)(\\mathbf{X}w-y)$\n",
    "\n",
    "$\\hspace{79pt}=\\frac{1}{2}(w^T\\mathbf{X}^T(\\mathbf{X}w-y)-y^T(\\mathbf{X}w-y))$\n",
    "\n",
    "$\\hspace{79pt}=\\frac{1}{2}(w^T\\mathbf{X}^T\\mathbf{X}w-w^T\\mathbf{X}^Ty-y^T\\mathbf{X}w+y^Ty)$\n",
    "\n",
    "$\\hspace{79pt}=\\frac{1}{2}(w^T\\mathbf{X}^T\\mathbf{X}w-((\\mathbf{X}^Ty)^Tw)_{1\\times 1}^T-(\\mathbf{X}^Ty)^Tw_{1\\times 1}+y^Ty)$\n",
    "\n",
    "$\\hspace{79pt}=\\frac{1}{2}(w^T\\mathbf{X}^T\\mathbf{X}w-(\\mathbf{X}^Ty)^Tw+y^Ty)$\n",
    "\n",
    "This boils the objective function down to:\n",
    "* A quadratic term (symmetric matrix), $\\frac{1}{2}(\\mathbf{X}\\bar{w})^T\\mathbf{X}\\bar{w}$\n",
    "* A linear term, $-\\frac{1}{2}\\mathbf{X}^T\\bar{y}^T\\bar{w}$\n",
    "* A constant $\\frac{1}{2}\\bar{y}^T\\bar{y}$\n",
    "\n",
    "We now compute the gradient as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\bar{w}}\\mathcal{f}(\\bar{w}) = \\frac{\\partial}{\\partial\\bar{w}}\\frac{1}{2}(\\bar{w}^T\\mathbf{X}^T\\mathbf{X}\\bar{w}-(\\mathbf{X}^T\\bar{y})^T\\bar{w}+\\bar{y}^T\\bar{y})$\n",
    "\n",
    "$\\hspace{5pt}\\nabla \\mathcal{f}(\\bar{w})=\\mathbf{X}^T\\mathbf{X}\\bar{w}-\\mathbf{X}^T\\bar{y}$\n",
    "\n",
    "Setting gradient to zero to find the minimize it:\n",
    "\n",
    "$\\mathbf{X}^T\\mathbf{X}\\bar{w}-\\mathbf{X}^T\\bar{y}=0$\n",
    "\n",
    "$(\\mathbf{X}^T\\mathbf{X})\\bar{w} = \\mathbf{X}^T\\bar{y}$\n",
    "\n",
    "$(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X})\\bar{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\bar{y}$\n",
    "\n",
    "**NOTE** Here, we assume that $\\mathbf{X}^T\\mathbf{X}$ is invertible\n",
    "\n",
    "$\\therefore\\bar{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\bar{y}$\n",
    "\n",
    "This is the **Least Squares Solution**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
