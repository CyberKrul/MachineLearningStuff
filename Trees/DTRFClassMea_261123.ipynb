{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d428a4",
   "metadata": {},
   "source": [
    "# Decision Trees, Random Forests and Performance Measures for Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4977880",
   "metadata": {},
   "source": [
    "## What is a Decision Tree?\n",
    "* It is a predictive model in the tree form that maps items to its target value, starting from the root to leaf (conclusion)\n",
    "* There are two types:\n",
    "    - Classification tree: When the final target value belongs to finite set (leaves are labels)\n",
    "    - Regression tree: when the final target value belongs to continuous set (different values based on features)\n",
    "* There are many algorithms like ID3, CART, CHAID, etc\n",
    "* Metrics for the above mentioned algorithms can be:\n",
    "    - Gini Impurity (GI)\n",
    "    - Information entropy (IE)\n",
    "    - variance reduction\n",
    "---\n",
    "Decision tree is a tree shaped diagram used to determine a course of action. Each branch of the tree represents a possible decision, occurence or reaction.\n",
    "___\n",
    "\n",
    "![](bt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45f1e",
   "metadata": {},
   "source": [
    "## Important Terms in Decision Trees\n",
    "* **Entropy**<br> It is a measure of randomness or unpredictability in the dataset\n",
    "![](entropy.jpg)\n",
    "---\n",
    "* **Information Gain**<br> It is the measure of decrease in entropy after the dataset is split\n",
    "![](ig.png)\n",
    "---\n",
    "* **left node**<br> Carries the classification or the decision\n",
    "---\n",
    "* **Root node**<br> The top most decision node is known as the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25b934",
   "metadata": {},
   "source": [
    "## How does a Decision Tree work?\n",
    "Let's begin with an example:\n",
    "___\n",
    "**Problem statement**<br> To classify the different types of animals based on their features using decision tree\n",
    "___\n",
    "How do we split the data?<br>\n",
    "We have to frame the conditions that split the data in such a way that the information gain is the highest\n",
    "\n",
    "**NOTE**: Gain is the measure of decrease in entropy after splitting\n",
    "___\n",
    "\n",
    "Entropy is calculated as:\n",
    "<p style='text-align:center;'> Entropy$=-\\Sigma_{i=1}^{k}P(class_i)\\log_2(P(class_i))$</p>\n",
    "\n",
    "where,\n",
    "* $k$ is the number of classes in the dataset\n",
    "* $P(class_i)$ is the fraction of $class_i$ in the dataset\n",
    "---\n",
    "Say the initial dataset contains:\n",
    "|color|height|label|\n",
    "|-----|------|-----|\n",
    "|grey|10|elephant|\n",
    "|yellow|10|giraffe|\n",
    "|brown|3|monkey|\n",
    "|grey|10|elephant|\n",
    "|yellow|4|tiger|\n",
    "\n",
    "* the *Features* are height and color, and the *output variable* is the label/type of animal\n",
    "___\n",
    "Calculating the initial entropy for:\n",
    "|label|count|\n",
    "|-----|-----|\n",
    "|giraffe|3|\n",
    "|tiger|2|\n",
    "|monkey|1|\n",
    "|elephant|2|\n",
    "|-----|-----|\n",
    "|total|8|\n",
    "\n",
    "using the formula for entropy, we get:\n",
    "\n",
    "$Entropy = -\\frac{3}{8}\\log_2(\\frac{3}{8})-\\frac{2}{8}\\log_2(\\frac{2}{8})-\\frac{1}{8}\\log_2(\\frac{1}{8})-\\frac{2}{8}\\log_2(\\frac{2}{8})=1.904$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406a37e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9056390622295665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#data\n",
    "A = np.array([3,2,1,2])\n",
    "prob = A/np.sum(A)\n",
    "\n",
    "#entropy calculations\n",
    "entropy = np.sum(-prob*np.log2(prob))\n",
    "\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cede6a",
   "metadata": {},
   "source": [
    "* Gain can be calculated by finding the difference of the subsequent entropy values after split\n",
    "* We will calculate the entropy of the dataset similarly after every split to calculate the gain\n",
    "\n",
    "___\n",
    "* Now we will try and choose a condition that gives us the highest gain\n",
    "* We will do that by splitting the data using each condition and checking the gain that we get out of them\n",
    "* The condition that gives us the highest gain will be used to make the first split\n",
    "___\n",
    "* We will split the data on the basis of the condition \"Color==yellow\"\n",
    "* Calculating the entropy of the two groups as:\n",
    "1. Group One: (Color == Yellow) == True\n",
    "|label|count| \n",
    "|-----|-----| \n",
    "|giraffe|3|   \n",
    "|tiger|2|\n",
    "|-----|-----|\n",
    "|total|5|\n",
    "\n",
    "$Entropy = -\\frac{2}{5}\\log_2(\\frac{2}{5})-\\frac{3}{5}\\log_2(\\frac{3}{5})=.97$\n",
    "\n",
    "2. Group Two: (Color == Yellow) == False\n",
    "|label|count|\n",
    "|-----|-----|\n",
    "|monkey|1|\n",
    "|elephant|2|\n",
    "|-----|-----|\n",
    "|total|3|\n",
    "\n",
    "$Entropy = -\\frac{1}{3}\\log_2(\\frac{1}{3})-\\frac{2}{3}\\log_2(\\frac{2}{3})=.92$\n",
    "\n",
    "___\n",
    "Now we will calculate the change in entropy as the difference of the entropy of the parent node and the entropy of the child nodes weighted by size of the datapoints absorbed by the resp node\n",
    "\n",
    "Information Gain $(IG) = 1.905-\\frac{5}{8}0.97-\\frac{3}{8}0.92=0.953$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30579c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.9182958340544896\n",
      "0.9537949406953985\n"
     ]
    }
   ],
   "source": [
    "# group 1\n",
    "A = np.array([2,3])\n",
    "prob1 = A/np.sum(A)\n",
    "\n",
    "ent1 = np.sum(-prob1*np.log2(prob1))\n",
    "print(ent1)\n",
    "\n",
    "# group 1\n",
    "B = np.array([1,2])\n",
    "prob2 = B/np.sum(B)\n",
    "\n",
    "ent2 = np.sum(-prob2*np.log2(prob2))\n",
    "print(ent2)\n",
    "\n",
    "#info gain\n",
    "IG = 1.905-ent1*np.sum(A)/8-ent2*np.sum(B)/8\n",
    "print(IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b3d0e",
   "metadata": {},
   "source": [
    "We can still furthur split the data on the basis of height, and obtain the entropy value zero, which is the **lowest possible value of entropy**\n",
    "* Now since every branch now contains single label type, we can say that the entropy in this case has reached the least value\n",
    "* this tree can now predict all the classes of animals present in the dataset with 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a3223",
   "metadata": {},
   "source": [
    "## Metrics for the decision tree\n",
    "* Let us consider Gini impurity as the metric,\n",
    "    - Gini Impurity $GI = 1-\\Sigma_{i=1}^m\\mathcal{f}_i^2$\n",
    "    - Gini split index $=GI(s)-p_1GI(s_1)-p_2(GIs_2)$\n",
    "    - Information entropy $IE = -\\Sigma_{i=1}^{m}\\mathcal{f}_i\\log_2(\\mathcal{f}_i)$\n",
    "* Where,\n",
    "    - $\\mathcal{f}_i$ is the fraction of class label $i$\n",
    "    - $s$ is the parent node, $s_1$ and $s_2$ are the child nodes\n",
    "    - $p_1$ and $p_2$ are split fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc9716",
   "metadata": {},
   "source": [
    "## Important Rules for constructing Trees\n",
    "* Every parent node of higher Gini impurity/information entropy is split based on features in order to lower its Gini impurity (or information entropy or variance reduction in the case of regression trees)\n",
    "* Gini impurity of pure sets = 0\n",
    "* The split which corresponds to higher Gini Split index is always preferred\n",
    "* *example* \n",
    "    - If 'split index One' $=.5$ and 'split index Two' $= .25$, then split corresponding to 'split index 1' will be chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61736714",
   "metadata": {},
   "source": [
    "## Fischer's Iris dataset\n",
    "* the problem:\n",
    "    - Discriminate between three different species of Iris flowers\n",
    "* The training data contains the following species by number:\n",
    "    - 49 setosa\n",
    "    - 50 versicolor\n",
    "    - 50 virginica  \n",
    "* The features that are available are:\n",
    "    - sepal length(cm)\n",
    "    - sepal width\n",
    "    - petal length\n",
    "    - petal width\n",
    "* The ranges for these features are given below\n",
    "| |setosa|versicolor|virginica| \n",
    "|-----|-----|-----|------------|\n",
    "|sepal Len|[4.3,5.8]|[4.9,7]|[4.9,7.9]| \n",
    "|sepal wid|[2.3,4.4]|[2,3.4]|[2.2,3.8]|\n",
    "|petal Len|[1,1.9]|[3.5,1]|[4.5,6.9]|\n",
    "|petal wid|[0.1,0.6]|[1,1.8]|[1.4,2.5]|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a565",
   "metadata": {},
   "source": [
    "## Construction of Nodes for the Iris dataset (Level 1)\n",
    "The training data contains 49 setosa, 50 versicolor, and 50 virginica species.\n",
    "* The root node could start with versicolor\n",
    "\n",
    "---\n",
    "#### Possibility One\n",
    "* By choosing petal length as splitting feature:\n",
    "    - 2.4 is considered as the mid-point and the splitting criteria\n",
    "    - In doing do we can split setosa in to a completely pure dataset\n",
    "    - Here,\n",
    "        * $s$ is the parent node with all 3 species\n",
    "        * $s_1$ is all values with petal length less than 2.4\n",
    "        * $s_2$ is all values with petal length greater than 2.4\n",
    "        * $p_1=\\frac{49}{149}$ and $p_2 = \\frac{100}{149}$ are the splitting fractions\n",
    "    \n",
    "$GI(s) = 1-(\\frac{49}{149})^2-(\\frac{50}{149})^2-(\\frac{50}{149})^2=.66$\n",
    "\n",
    "$GI(s_1) = 1-(\\frac{49}{49})^2 = 0$\n",
    "\n",
    "$GI(s_2) = 1-(\\frac{50}{100})^2-(\\frac{50}{100})^2=.5$\n",
    "\n",
    "The Gini split index is calculated as:\n",
    "\n",
    "$GSI(s,(s_1,s_2)) = .66-\\frac{49}{149}0-\\frac{100}{149}0.5=0.324$\n",
    "___\n",
    "#### Possibility Two\n",
    "* By choosing sepal length as splitting feature:\n",
    "    - 5.8 is considered as the mid-point and the splitting criteria\n",
    "    - As range values overlap, we must consider the edge value that corresponds to high split index\n",
    "    - 4.9 is also a decent choice for splitting point for sepal length\n",
    "    - Here,\n",
    "        * $s$ is the parent node with all 3 species\n",
    "        * $s_1$ is all values with sepal length less than 5.8\n",
    "        * $s_2$ is all values with sepal length greater than 5.8\n",
    "        * $p_1=\\frac{73}{149}$ and $p_2 = \\frac{76}{149}$ are the splitting fractions\n",
    "    \n",
    "$GI(s) = 1-(\\frac{49}{149})^2-(\\frac{50}{149})^2-(\\frac{50}{149})^2=.66$\n",
    "\n",
    "$GI(s_1) = 1-(\\frac{49}{73})^2-(\\frac{21}{73})^2-(\\frac{3}{73})^2 = .465$\n",
    "\n",
    "$GI(s_2) = 1-(\\frac{29}{76})^2-(\\frac{47}{76})^2=.472$\n",
    "\n",
    "The Gini split index is calculated as:\n",
    "\n",
    "$GSI(s,(s_1,s_2)) = .66-\\frac{73}{149}0.465-\\frac{76}{149}0.472=0.1915$\n",
    "\n",
    "If we took the splitting value of sepal length as $4.9$, then the $GSI$ would equal $0.0746$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d1719",
   "metadata": {},
   "source": [
    "## Construction of Nodes for the Iris dataset (Level 2)\n",
    "* We have chosen the possibility with the higher $GSI$ and that was possibility one (splitting by petal length, by 2.4)\n",
    "* By choosing petal width as splitting feature at level 2:\n",
    "    - 1.8 is considered as the mid-point and the splitting criteria\n",
    "    - As range values overlap, we must consider the edge value that corresponds to high split index\n",
    "    - 1.4 is also a decent choice for splitting point for sepal length\n",
    "    - Here,\n",
    "        * $s$ is $s_2$ from level 1, with two species\n",
    "        * $s_1$ is all values with sepal length less than 5.8\n",
    "        * $s_2$ is all values with sepal length greater than 5.8\n",
    "        * $p_1=\\frac{54}{100}$ and $p_2 = \\frac{46}{100}$ are the splitting fractions\n",
    "    \n",
    "$GI(s) = 1-(\\frac{50}{100})^2-(\\frac{50}{100})^2=.5$\n",
    "\n",
    "$GI(s_1) = 1-(\\frac{49}{54})^2-(\\frac{5}{54})^2 = .168$\n",
    "\n",
    "$GI(s_2) = 1-(\\frac{1}{46})^2-(\\frac{45}{46})^2=.0425$\n",
    "\n",
    "The Gini split index is calculated as:\n",
    "\n",
    "$GSI(s,(s_1,s_2)) = .5-\\frac{54}{100}0.168-\\frac{46}{100}0.0425=0.3897$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3eb69",
   "metadata": {},
   "source": [
    "![](irisdct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8c270",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "* It is an ensemble technique that bags decision trees from multiple subsets of given data.\n",
    "* It is used for regression / classification problems\n",
    "* It aims to reduce overfitting to the training dataset\n",
    "* The algorithm consists of two parts:\n",
    "    - Split the data set into many subsets based on its features and then build a decision tree classifier\n",
    "    - Bag all the classifiers obtained from every subset and classify the test data\n",
    "    - Based on voting or average method classify the data\n",
    "![](randfor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46440560",
   "metadata": {},
   "source": [
    "## Final thoughts on RFs\n",
    "* Random Forest$=\\mathcal{f}(data,D_1,D_2,\\dots,D_n)$\n",
    "* where\n",
    "    - $\\mathcal{f}$ is the voting/averaging function\n",
    "    - $data$ is the test data\n",
    "    - $D_1,D_2,\\dots,D_n$ are the data subsets provided to the trees of the random forest\n",
    "* Say, the final decision observed form $n$ different subsets using $GSI$ split methods is {'setosa','setosa','versicolor'}\n",
    "* $\\therefore$ according to the voting method, random forest function classifies the test data as 'setosa'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bb85a",
   "metadata": {},
   "source": [
    "# Performance Measures for Classification tasks\n",
    "* Terminology:\n",
    "    - $TP$ -True positives\n",
    "    - $TN$ -True Negatives\n",
    "    - $FP$ -False positives\n",
    "    - $FN$ -False Negatives\n",
    "    - $N = TP+TN + FP+FN$\n",
    "    \n",
    "![](tpfn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4df1c2",
   "metadata": {},
   "source": [
    "## Measures of Accuracy\n",
    "* **ACCURACY**<br> Overall effectiveness of a classifier\n",
    "    - $A = \\frac{TP+TN}{N}$\n",
    "    - the highest value that accuracy can reach is 1\n",
    "    - this happens when the classifier exactly classifies two groups $(FP=0, FN=0)$\n",
    "* Remember, Total number of True positive labels $=TP+FN$\n",
    "* Total number of True negative labels $=TN+FP$\n",
    "---\n",
    "* Accuracy is a good measure when the classes in the dataset are nearly balanced\n",
    "    - *eg* All the classes of flowers in iris have same number of data points\n",
    "* Accuracy is useful when the target class is **well balanced** but is not a good choice for unbalanced classes\n",
    "    - Imagine the scenario where we had 99 images of the dog and only 1 image of a cat present in our training data. Then our modle would always predict a dog with almost 100% accuracy, and this would lead to high overall accuracy\n",
    "    - In reality, data is always imbalanced for example spam email, credit card fraud, and medical diagnosis\n",
    "    - Hence, if we want a better model evaluation and have a full picture of the model's performance, other metrics such as recall and precision should be employed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c25533",
   "metadata": {},
   "source": [
    "## Recall, Precision\n",
    "* Consider we have 50000 passengers travel per day on average. Out of which, 10 are actually COVID positive\n",
    "* One of the easy ways to increase accuray is to classify every passenger as covid negative So the confusion matrix looks like:\n",
    "\n",
    "|predicted|actual positive|actual negative|\n",
    "|-----|-----|-----------------------------|\n",
    "|Positive|TP=0|FP=0|\n",
    "|Negative|FN=10|TN=49990|\n",
    "\n",
    "* The accuracy for this case will be Accuracy = $\\frac{49990}{50000}=> 99.98\\%$ Accuracy\n",
    "* However this does nothing to solve our problem of correctly identifying covid19-positive customers\n",
    "\n",
    "### Recall\n",
    "<p style='text-align:center;'>Recall $=\\frac{TP}{TP+FN}$</p>\n",
    "\n",
    "* This is simply the ratio of correctly predicted Covid19-positive passengers($TP$) to the total number of True Covid19-positive passengers ($TP+FN$)\n",
    "* For our example, our recall comes out to be zero\n",
    "* **NOTE** In this context, Recall is a good measure. It says that the terrible strategy of identifying every passenger as covid19-negative leads to zero recall, while we want to maximize recall.\n",
    "___\n",
    "* Consider another scenario of labelling every passenger as covid19-positive.\n",
    "* The model labels all passengers as covid19-positive\n",
    "* Labelling every passenger as positive is bad in terms of the amount of cost that needs to be spent in actually investigating each passenger before they board the flight\n",
    "* Our Recall in this scenario comes to $1$, which is the highest possible value but this model is still unreliable\n",
    "* Hence recall is not a good measure by itself\n",
    "---\n",
    "### Precision\n",
    "<p style='text-align:center;'>Precision $=\\frac{TP}{TP+FP}$</p>\n",
    "\n",
    "* This is simply the ratio of correctly predicted Covid19-positive passengers($TP$) to the total number of predicted Covid19-positive passengers ($TP+FP$)\n",
    "* For our example, our Precision comes out to be miniscule\n",
    "* **NOTE** This bad strategy with good recall has a terrible precision value, while we want maximum precision. This shows that recall alone is not a good measure, we also need to consider precision and vice-versa\n",
    "___\n",
    "\n",
    "### $F_1$ Score\n",
    "* It is the harmonic mean of precision and recall/sensitivity\n",
    "* $F_1 = \\frac{\\frac{1}{precision}+\\frac{1}{recall}}{2} = \\frac{2\\times precision\\times recall}{precision+recall}$\n",
    "* $F_1\\in[0,1]$\n",
    "* The higher the $F_1$ score, the better, with zero being the worst possible and 1 being the best\n",
    "\n",
    "---\n",
    "### Other Measures\n",
    "* **Sensitivity**: This is recall, ratio of True positive to actual positive\n",
    "* **Specificity**: This is like recall for negatives; ratio of True negative to actual negative\n",
    "* Both Sensitivity and Specificity $\\in [0,1]$, with 1 being the ideal value\n",
    "\n",
    "* **Precision**: ratio of True positives to predicted positives\n",
    "\n",
    "___\n",
    "### Other measures of Accuracy\n",
    "* Observed Accuracy\n",
    "<p style='text-align:center;'>$OA = \\frac{TP+TN}{N}$ </p>\n",
    "\n",
    "* Expected Accuracy or Chance Agreement\n",
    "<p style='text-align:center;'>$EA = \\frac{(TP+FN)(TP+FP)+(TN+FN)(TN+FP)}{N}$ </p>\n",
    "\n",
    "* Kappa\n",
    "<p style='text-align:center;'>$EA = \\frac{OA-EA}{1-EA}$ </p>\n",
    "\n",
    "![](kappa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c397b79",
   "metadata": {},
   "source": [
    "### ROC\n",
    "* It is an acronym for Receiver Operating Characteristics\n",
    "* Originally used in signal detection theory\n",
    "* ROC graph:\n",
    "    - sensitivity as function of specificity\n",
    "    - sensitivity (Y-axis) and 1-specificity(X-axis)\n",
    "* ROC can be used to see the classifier performance at different threshold levels (from zero to one)\n",
    "* AUC is the Area Under Curve (of the ROC)\n",
    "    - An area of $1$ represents a perfect test; an area of $0.5$ represents a terrible model\n",
    "    - $(0.9,1]$ is an excellent AUC value\n",
    "    - $(0.8,0.9]$ is good\n",
    "    - $(0.7,0.8]$ is okay\n",
    "    - $(0.6,0.7]$ is poor\n",
    "* If $AUC\\lt 0.5$, check whether your labels are marked in the opposite direction before discarding the model\n",
    "    \n",
    "![](roc.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eea38",
   "metadata": {},
   "source": [
    "* The ROC can also be used to compare different classifiers at one threshold or overall threshold levels\n",
    "![](modper.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
